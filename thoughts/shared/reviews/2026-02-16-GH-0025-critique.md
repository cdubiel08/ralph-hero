---
date: 2026-02-16
github_issue: 25
github_url: https://github.com/cdubiel08/ralph-hero/issues/25
plan_document: thoughts/shared/plans/2026-02-16-GH-0025-auto-estimation-engine.md
status: approved
type: critique
---

# Plan Critique: GH-25 Auto-estimation Engine for Heuristic Issue Sizing

## Verdict: APPROVED with warnings

The plan is well-structured, technically sound, and implementable as written. The two-phase architecture (pure engine in `lib/estimation-engine.ts` + MCP tool wrapper in `tools/estimation-tools.ts`) is clean and testable. All referenced files exist, line numbers are accurate, and the GraphQL field names match the existing codebase. Five warnings are noted below -- none are blocking, but the scoring calibration and unused field issues should be addressed during implementation.

## Codebase Verification Results

All technical claims verified against the actual codebase:

| Claim | Status | Detail |
|-------|--------|--------|
| `update_estimate` at issue-tools.ts:1298-1349 | VERIFIED | Lines 1297-1349, tool registered with `update_estimate` name |
| `get_issue` at issue-tools.ts:585-879 | VERIFIED | Tool starts at line 588, GraphQL query at lines 668-721 |
| `pick_actionable_issue` at issue-tools.ts:1677-1906 | VERIFIED | Exact lines match |
| `validEstimates.indexOf()` at line 1714 | VERIFIED | Line 1714: `const validEstimates = ["XS", "S", "M", "L", "XL"];` |
| `ESTIMATE_OPTIONS` at project-tools.ts:94-100 | VERIFIED | XS/S/M/L/XL with correct colors and descriptions |
| `OVERSIZED_ESTIMATES` at pipeline-detection.ts:70 | VERIFIED | `new Set(["M", "L", "XL"])` at line 70 |
| `lib/estimation-engine.ts` does not exist | VERIFIED | File confirmed absent |
| `tools/estimation-tools.ts` does not exist | VERIFIED | File confirmed absent |
| `lib/helpers.ts` does not exist | VERIFIED | File confirmed absent (planned in #21 but not yet implemented) |
| `resolveConfig` not exported from issue-tools.ts | VERIFIED | Function defined at line 329 as `function resolveConfig(...)`, no `export` keyword |
| Tool registration pattern in index.ts | VERIFIED | Lines 283-294 show phase-based registration; new tools go after line 294 |

### GraphQL Field Verification

The plan's Phase 2 GraphQL query uses the following fields. Each was checked against the existing `get_issue` query at issue-tools.ts:668-721:

| Plan's Field | Actual Field in get_issue | Match |
|--------------|--------------------------|-------|
| `subIssuesSummary { total }` | `subIssuesSummary { total completed percentCompleted }` (line 685) | MATCH |
| `trackedInIssues(first: 1) { totalCount }` | `trackedInIssues(first: 20) { nodes { number title state } }` (lines 689-691) | PARTIAL -- see Warning #1 |
| `trackedIssues(first: 1) { totalCount }` | `trackedIssues(first: 20) { nodes { number title state } }` (lines 692-694) | PARTIAL -- see Warning #1 |
| `comments(last: 1) { totalCount }` | `comments(last: 10) { nodes { id body author { login } createdAt } }` (lines 695-700) | PARTIAL -- see Warning #1 |
| `labels(first: 20) { nodes { name } }` | `labels(first: 20) { nodes { name color } }` (line 682) | MATCH |
| `projectItems(first: 10) { ... }` | `projectItems(first: 10) { ... }` (lines 703-718) | MATCH |

## Warnings

### 1. GraphQL `totalCount` Field Availability (Low Risk)

The plan's Phase 2 query requests `totalCount` on `trackedInIssues(first: 1)`, `trackedIssues(first: 1)`, and `comments(last: 1)`. The existing `get_issue` query does NOT use `totalCount` on these connections -- it fetches `nodes` instead.

GitHub GraphQL connections DO support `totalCount` alongside `nodes`, `edges`, and `pageInfo`, so the plan's approach will work. However, the plan asks for `first: 1` to minimize data transfer while getting `totalCount`. This is valid -- `totalCount` returns the full count regardless of pagination limits.

**Impact**: None -- the plan's approach is actually more efficient than fetching all nodes just to count them. Noted only because it deviates from the existing query pattern and the implementer should verify the field works as expected with a test call.

### 2. `commentCount` Fetched But Unused in Signal Extractors (Medium)

The `IssueData` interface includes `commentCount` (plan line 110), and the Phase 2 tool fetches it from `comments.totalCount` (plan line 298). However, **none of the 8 signal extractors use `commentCount`**:

1. `extractBodyLength(body)` -- uses body
2. `extractCheckboxCount(body)` -- uses body
3. `extractCodeBlockCount(body)` -- uses body
4. `extractSectionCount(body)` -- uses body
5. `extractFilePathCount(body)` -- uses body
6. `extractKeywords(title, body)` -- uses title and body
7. `extractLabelSignals(labels)` -- uses labels
8. `extractRelationshipSignals(subIssueCount, dependencyCount)` -- uses subIssueCount and dependencyCount

The research document (line 56) explicitly identified comment volume as a complexity signal: "Discussion volume suggests ambiguity/complexity." Yet the plan defines the field, fetches the data, and then never uses it.

**Recommendation**: Either (a) add a 9th signal extractor `extractCommentSignals(commentCount)` with a simple heuristic (e.g., 0-2 comments = neutral, 3-5 = +0.5, 6+ = +1 suggesting scope uncertainty), or (b) remove `commentCount` from `IssueData` and the GraphQL query to avoid dead code. Option (a) is preferable since comment volume genuinely signals complexity. This is not blocking -- the engine works fine without it, but it's a missed signal from the plan's own research.

### 3. Scoring Calibration Is Loose but Acceptable (Medium)

Analysis of the weight ranges and score-to-estimate mapping:

**Maximum possible rawScore** (all signals fire at max positive):
- bodyLength: +1 (>1000 chars)
- checkboxes: +1.5 (9+)
- codeBlocks: +1 (4+)
- sections: +1 (5+)
- filePaths: +1 (5+)
- keywords: +2 (capped)
- labels: +1.5 (breaking-change)
- relationships: +1.5 (sub-issues + deps)

**Total max: +10.5**

**Minimum possible rawScore** (all signals fire at max negative):
- bodyLength: -1 (<200 chars)
- checkboxes: 0 (neutral floor)
- codeBlocks: 0
- sections: 0
- filePaths: 0
- keywords: -2 (capped)
- labels: -1 (documentation)
- relationships: 0

**Total min: -4.0**

**Score-to-estimate thresholds**:
- XS: rawScore <= -1.0
- S: -1.0 to 0.5
- M: 0.5 to 2.0
- L: 2.0 to 3.5
- XL: >3.5

**Observations**:

1. **XL is achievable**: A complex issue with long body (+1), 9+ checkboxes (+1.5), many code blocks (+1), many sections (+1), many file paths (+1), "refactor" + "database" keywords (+2), "breaking-change" label (+1.5), sub-issues (+1) = +10.0, well into XL. But it requires an unusually feature-rich issue description. Realistic XL-worthy issues may not describe themselves this thoroughly.

2. **M is easy to reach**: An issue with a medium body (+0.5), 4 checkboxes (+0.5), one complexity keyword (+1) = +2.0, which hits M threshold. This seems reasonable for medium-complexity work.

3. **S band is narrow**: The S range (-1.0 to 0.5) spans only 1.5 points, while M (0.5 to 2.0) spans 1.5 and L (2.0 to 3.5) also spans 1.5. The ranges are actually evenly distributed, which is fine.

4. **Default/empty issues land in S**: An issue with a moderate body (200-500 chars, weight 0), no checkboxes (0), no labels (0), no relationships (0), no keywords (0) scores 0.0 = S. This is a reasonable default for an under-specified issue.

5. **The "documentation" label + short body combination correctly produces XS**: Body <200 chars (-1) + "documentation" label (-1) + possible "typo" keyword (-1) = -3.0, well into XS.

**Conclusion**: The calibration is reasonable for an MVP heuristic engine. The plan acknowledges calibration tuning as future work, and the transparent signal breakdown allows manual verification. Not blocking.

### 4. Confidence Calculation Is Under-specified (Low)

The plan says confidence is calculated as `1.0 - (stddev of weights / max possible stddev)`, clamped to [0.3, 0.95]. Two issues:

1. **"Max possible stddev" is not defined**: The plan doesn't specify what population this refers to. If it means the stddev of all 8 signal weights at their theoretical extremes, this is a fixed constant that should be pre-computed. If it means the stddev of non-zero weights only, it varies per issue. The implementer needs to decide.

2. **Single non-zero signal edge case**: If only 1 signal fires (e.g., a short body with no other features), stddev of a single value is 0, making confidence = 1.0 (clamped to 0.95). This is counterintuitive -- confidence should be LOW when only one signal contributes, since there's insufficient evidence. The plan's formula produces high confidence from sparse data.

**Recommendation**: The implementer should consider an alternative approach: factor in the NUMBER of non-zero signals as a confidence multiplier. For example: `base_confidence = 1.0 - (stddev / max_stddev)`, then `final_confidence = base_confidence * min(nonZeroSignalCount / 4, 1.0)`, clamped to [0.3, 0.95]. This penalizes sparse signal sets while rewarding signal agreement. Not blocking -- the plan's approach works, just produces misleadingly high confidence for sparse issues.

### 5. #21 Coordination: `resolveConfig` Duplication (Low Risk)

The plan recommends inlining a minimal `resolveConfig` in `estimation-tools.ts` to avoid modifying `issue-tools.ts`. However, GH-21's Phase 1 plans to extract `resolveConfig` (and 5 other helpers) into `lib/helpers.ts`. If #21 lands first, the estimation tool should import from `lib/helpers.ts` instead of inlining.

**Current state**: `lib/helpers.ts` does not exist yet. GH-21's plan is approved but not implemented.

**Recommendation**: The plan's inline approach is pragmatic for now. If #21 lands first during implementation, the implementer should import from `lib/helpers.ts` instead. If #25 lands first, the inline version creates a small duplication that #21 will clean up. Either ordering works without conflicts. Not blocking.

## What's Good

- **Clean two-layer architecture**: Pure engine (zero I/O dependencies) + tool wrapper (I/O) is an excellent separation of concerns that maximizes testability
- **Accurate line references**: All 8 referenced file locations verified correct within 1-2 lines
- **Comprehensive signal design**: 8 signal extractors cover body metrics, keywords, labels, and relationships -- all the major dimensions identified in research
- **Transparent signal reporting**: Returning `factor`, `value`, `impact`, and `weight` for each signal makes debugging and trust-building easy
- **Correct GraphQL field names**: `subIssuesSummary`, `trackedInIssues`, `trackedIssues`, `labels`, `comments`, `projectItems` all match the existing schema
- **Well-bounded scope**: Clear "What We're NOT Doing" section excludes historical calibration, auto-application, NLP, and triage skill integration
- **Test strategy is thorough**: Individual signal extractor tests, end-to-end scoring tests, edge cases (empty body, no labels), and confidence direction tests
- **Good integration with existing infrastructure**: `oversized` flag ties directly to `OVERSIZED_ESTIMATES` from pipeline-detection.ts
- **Sensible defaults**: Empty/under-specified issues default to S (not XS), which is conservative given that M+ triggers SPLIT

## Summary

The plan is ready for implementation. The two warnings worth noting during implementation are: (1) `commentCount` is fetched but unused -- add a 9th signal extractor or remove the field, and (2) the confidence formula produces misleadingly high confidence for sparse signal sets. Neither is blocking. The scoring calibration, while loose, is acceptable for an MVP and the plan correctly defers fine-tuning to future work. The implementer should watch for #21 landing and import `resolveConfig` from `lib/helpers.ts` if available.
